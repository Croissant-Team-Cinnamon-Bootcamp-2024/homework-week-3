{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip uninstall clip\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import roboflow\n",
    "\n",
    "roboflow.login()\n",
    "\n",
    "roboflow.download_dataset(dataset_url=\"https://universe.roboflow.com/team-roboflow/coco-128/dataset/2\", model_format=\"coco\", location=\"data/coco-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(embeddings):\n",
    "    # Determine the dimensionality of the embeddings\n",
    "    d = 512\n",
    "    \n",
    "    # Initialize a FAISS index\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "\n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "def load_faiss_index(index_path, image_paths_path):\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load image paths\n",
    "    with open(image_paths_path, \"r\") as f:\n",
    "        image_paths = json.load(f)\n",
    "    \n",
    "    return index, image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import clip\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, preprocess):\n",
    "        self.image_paths = image_paths\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = self.preprocess(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "def get_data_paths(dir: str | list[str], data_formats: list, prefix: str = '') -> list[str]:\n",
    "    \"\"\"\n",
    "    Get list of files in a folder that have a file extension in the data_formats.\n",
    "\n",
    "    Args:\n",
    "      dir (str | list[str]): Dir or list of dirs containing data.\n",
    "      data_formats (list): List of file extensions. Ex: ['jpg', 'png']\n",
    "      prefix (str): Prefix for logging messages.\n",
    "\n",
    "    Returns:\n",
    "      A list of strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = []  # data files\n",
    "        for d in dir if isinstance(dir, list) else [dir]:\n",
    "            p = Path(d)\n",
    "            if p.is_dir():\n",
    "                f += glob.glob(str(p / '**' / '*.*'), recursive=True)\n",
    "            else:\n",
    "                raise FileNotFoundError(f'{prefix}{p} does not exist')\n",
    "        data_files = sorted(x for x in f if x.split('.')[-1].lower() in data_formats)\n",
    "        return data_files\n",
    "    except Exception as e:\n",
    "        raise Exception(f'{prefix}Error loading data from {dir}: {e}') from e\n",
    "\n",
    "\n",
    "def get_image_embeddings(data_dir, model_name=\"ViT-B/32\", batch_size=32, device=\"cpu\"):\n",
    "    # Load the CLIP model\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    \n",
    "    # Create a dataset and dataloader\n",
    "    image_paths = get_data_paths(data_dir, data_formats=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    print(len(image_paths))\n",
    "    dataset = ImageDataset(image_paths, preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # List to store image embeddings\n",
    "    image_embeddings = []\n",
    "\n",
    "    # Process images in batches\n",
    "    with torch.no_grad():\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = model.encode_image(images)\n",
    "            embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "            image_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    \n",
    "    return image_embeddings, image_paths\n",
    "\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    # Determine the dimensionality of the embeddings\n",
    "    d = 512\n",
    "    \n",
    "    # Initialize a FAISS index\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    \n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "embeddings, image_paths = get_image_embeddings(\"data/coco-128/train\", device=\"cpu\")\n",
    "# index = create_faiss_index(embeddings)\n",
    "# faiss.write_index(index, \"data/index.faiss\")\n",
    "# with open(\"data/image_paths.json\", \"w\") as f:\n",
    "#     json.dump(image_paths, f, indent=4)\n",
    "    \n",
    "filepath_mapping = {idx: path for idx, path in enumerate(image_paths)}\n",
    "with open(\"data/filepath.json\", \"w\") as f:\n",
    "    json.dump(filepath_mapping, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_images(images, index, image_paths, top_k=5, model_name=\"ViT-B/32\", device=\"cpu\"):\n",
    "    # Load the CLIP model\n",
    "    clip_model, clip_preprocess = clip.load(model_name, device=device)\n",
    "    \n",
    "    # Preprocess and get embeddings for input images\n",
    "    processed_images = torch.stack([clip_preprocess(img) for img in images]).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = clip_model.encode_image(processed_images)\n",
    "        embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "        embeddings = embeddings.cpu().numpy()\n",
    "    \n",
    "    # Search for similar images\n",
    "    D, I = index.search(embeddings, top_k)\n",
    "    \n",
    "    # Get the paths of the similar images\n",
    "    similar_images = [[image_paths[i] for i in indices] for indices in I]\n",
    "    print(similar_images)\n",
    "    similar_images = [[Image.open(img_path) for img_path in paths] for paths in similar_images]\n",
    "    \n",
    "    return similar_images\n",
    "\n",
    "# Example usage\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "index, image_paths = load_faiss_index(\"data/index.faiss\", \"data/image_paths.json\")\n",
    "\n",
    "\n",
    "query_images = [\"data/coco-128/test/000000000034_jpg.rf.b518abdaed199dcb88854cf20fce8078.jpg\", \"data/coco-128/test/000000000283_jpg.rf.27927692baf616a7456bb3e24c21bfd7.jpg\"]  # replace with your image paths\n",
    "query_images = [Image.open(img_path) for img_path in query_images]\n",
    "similar_images = get_similar_images(query_images , index, image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "INDEX_PATH = \"data/index.faiss\"\n",
    "IMAGE_PATHS_PATH = \"data/image_paths.json\"\n",
    "NUM_SAMPLES = 2  # Number of samples to retrieve\n",
    "\n",
    "def load_faiss_index(index_path, image_paths_path):\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    \n",
    "    # Load image paths\n",
    "    with open(image_paths_path, \"r\") as f:\n",
    "        image_paths = json.load(f)\n",
    "    \n",
    "    return index, image_paths\n",
    "\n",
    "def get_samples_from_index(index, image_paths, num_samples=NUM_SAMPLES):\n",
    "    # Get the total number of entries in the index\n",
    "    num_entries = index.ntotal\n",
    "    \n",
    "    # Select random indices to retrieve\n",
    "    sample_indices = np.random.choice(num_entries, num_samples, replace=False)\n",
    "    \n",
    "    # Retrieve the embeddings and corresponding image paths\n",
    "    sample_embeddings = []\n",
    "    sample_image_paths = []\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        sample_embeddings.append(index.reconstruct(int(idx)))\n",
    "        sample_image_paths.append(image_paths[int(idx)])\n",
    "    \n",
    "    return sample_embeddings, sample_image_paths\n",
    "\n",
    "def display_sample_images(sample_image_paths):\n",
    "    for image_path in sample_image_paths:\n",
    "        img = Image.open(image_path)\n",
    "        img.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the FAISS index and image paths\n",
    "    index, image_paths = load_faiss_index(INDEX_PATH, IMAGE_PATHS_PATH)\n",
    "    \n",
    "    # Get samples from the index\n",
    "    sample_embeddings, sample_image_paths = get_samples_from_index(index, image_paths)\n",
    "    \n",
    "    # Print sample embeddings and their corresponding image paths\n",
    "    for i, (embedding, path) in enumerate(zip(sample_embeddings, sample_image_paths)):\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"Image Path: {path}\")\n",
    "        print(f\"Embedding: {embedding}\")\n",
    "        print()\n",
    "\n",
    "    # Optionally display the images\n",
    "    display_sample_images(sample_image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import clip\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def get_image_embeddings(data_dir, model_name=\"ViT-B/32\", batch_size=32, device=\"cpu\"):\n",
    "    # Load the CLIP model\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    \n",
    "    # Create a dataset and dataloader\n",
    "    image_paths = get_data_paths(data_dir, data_formats=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    print(len(image_paths))\n",
    "    dataset = ImageDataset(image_paths, preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    # List to store image embeddings\n",
    "    image_embeddings = []\n",
    "\n",
    "    # Process images in batches\n",
    "    with torch.no_grad():\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = model.encode_image(images)\n",
    "            embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "            image_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    \n",
    "    return image_embeddings, image_paths\n",
    "\n",
    "embeddings, image_paths = get_image_embeddings(\"data/coco-128/train\", device=\"cpu\")\n",
    "\n",
    "# Create and save filepath mapping\n",
    "filepath_mapping = {idx: path for idx, path in enumerate(image_paths)}\n",
    "with open(\"data/filepath.json\", \"w\") as f:\n",
    "    json.dump(filepath_mapping, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from PIL import Image\n",
    "\n",
    "#Load CLIP model\n",
    "model = SentenceTransformer('clip-ViT-B-32')\n",
    "\n",
    "#Encode an image:\n",
    "img_emb = model.encode(Image.open('/Users/haphuongthao/Downloads/1a50811a14ee1a67b1fd7d7648fd24dd.png'))\n",
    "\n",
    "#Encode text descriptions\n",
    "text_emb = model.encode(['Two dogs in the snow', 'A cat on a table', 'A picture of London at night'])\n",
    "\n",
    "#Compute cosine similarities \n",
    "cos_scores = util.cos_sim(img_emb, text_emb)\n",
    "print(cos_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cinhw2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
