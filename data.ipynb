{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install roboflow\n",
    "import roboflow\n",
    "\n",
    "roboflow.login()\n",
    "\n",
    "roboflow.download_dataset(dataset_url=\"https://universe.roboflow.com/team-roboflow/coco-128/dataset/2\", model_format=\"coco\", location=\"data/coco-128\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [01:27<00:00, 4.03MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kieusontung/homework-week-3/w3_2/lib/python3.12/site-packages/torch/nn/modules/conv.py:456: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:84.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !pip install faiss-gpu\n",
    "# !pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "import glob\n",
    "import json\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "import clip\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, preprocess):\n",
    "        self.image_paths = image_paths\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path)\n",
    "        image = self.preprocess(image)\n",
    "        return image\n",
    "\n",
    "\n",
    "def get_data_paths(dir: str | list[str], data_formats: list, prefix: str = '') -> list[str]:\n",
    "    \"\"\"\n",
    "    Get list of files in a folder that have a file extension in the data_formats.\n",
    "\n",
    "    Args:\n",
    "      dir (str | list[str]): Dir or list of dirs containing data.\n",
    "      data_formats (list): List of file extensions. Ex: ['jpg', 'png']\n",
    "      prefix (str): Prefix for logging messages.\n",
    "\n",
    "    Returns:\n",
    "      A list of strings.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = []  # data files\n",
    "        for d in dir if isinstance(dir, list) else [dir]:\n",
    "            p = Path(d)\n",
    "            if p.is_dir():\n",
    "                f += glob.glob(str(p / '**' / '*.*'), recursive=True)\n",
    "            else:\n",
    "                raise FileNotFoundError(f'{prefix}{p} does not exist')\n",
    "        data_files = sorted(x for x in f if x.split('.')[-1].lower() in data_formats)\n",
    "        return data_files\n",
    "    except Exception as e:\n",
    "        raise Exception(f'{prefix}Error loading data from {dir}: {e}') from e\n",
    "\n",
    "\n",
    "def get_image_embeddings(data_dir, model_name=\"ViT-B/32\", batch_size=32, device=\"cpu\"):\n",
    "    # Load the CLIP model\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    \n",
    "    # Create a dataset and dataloader\n",
    "    image_paths = get_data_paths(data_dir, data_formats=[\"jpg\", \"jpeg\", \"png\"])\n",
    "    print(len(image_paths))\n",
    "    dataset = ImageDataset(image_paths, preprocess)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4)\n",
    "    \n",
    "    # List to store image embeddings\n",
    "    image_embeddings = []\n",
    "\n",
    "    # Process images in batches\n",
    "    with torch.no_grad():\n",
    "        for images in dataloader:\n",
    "            images = images.to(device)\n",
    "            embeddings = model.encode_image(images)\n",
    "            embeddings /= embeddings.norm(dim=-1, keepdim=True)\n",
    "            image_embeddings.append(embeddings.cpu().numpy())\n",
    "\n",
    "    # Convert list to numpy array\n",
    "    image_embeddings = np.vstack(image_embeddings)\n",
    "    \n",
    "    return image_embeddings, image_paths\n",
    "\n",
    "\n",
    "def create_faiss_index(embeddings):\n",
    "    # Determine the dimensionality of the embeddings\n",
    "    d = embeddings.shape[1]\n",
    "    \n",
    "    # Initialize a FAISS index\n",
    "    index = faiss.IndexFlatL2(d)\n",
    "    \n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings)\n",
    "    \n",
    "    return index\n",
    "\n",
    "\n",
    "embeddings, image_paths = get_image_embeddings(\"data/coco-128/train\", device=\"cuda\")\n",
    "index = create_faiss_index(embeddings)\n",
    "faiss.write_index(index, \"data/index.faiss\")\n",
    "with open(\"data/image_paths.json\", \"w\") as f:\n",
    "    json.dump(image_paths, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'clip' has no attribute 'available_models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mavailable_models\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'clip' has no attribute 'available_models'"
     ]
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 data/coco-128/train/000000000009_jpg.rf.856f80d728927e943a5bccfdf49dd677.jpg\n",
      "5 data/coco-128/train/000000000042_jpg.rf.a1f5c146b4b81881b19f342a148c0f51.jpg\n",
      "10 data/coco-128/train/000000000072_jpg.rf.244aee5e871d00c83983a224a5eb8ed5.jpg\n",
      "15 data/coco-128/train/000000000081_jpg.rf.5ac5126a29a5565691c27016453cb17b.jpg\n",
      "20 data/coco-128/train/000000000109_jpg.rf.a74c9fd0bdb672415629919c2c802f2d.jpg\n",
      "25 data/coco-128/train/000000000136_jpg.rf.71b51a4103c3e797f62a52a9d20fddfe.jpg\n",
      "30 data/coco-128/train/000000000149_jpg.rf.0a861d05b36be7927ab205acb325f4ce.jpg\n",
      "35 data/coco-128/train/000000000192_jpg.rf.ad225cb1bc09bfe56f6282c3f7ce56af.jpg\n",
      "40 data/coco-128/train/000000000241_jpg.rf.883e3e7bef174603fd5d2bbdbaf3a4ba.jpg\n",
      "45 data/coco-128/train/000000000263_jpg.rf.25c52657eff0a27882c0d1de8c72fe75.jpg\n",
      "50 data/coco-128/train/000000000309_jpg.rf.db7b22492fbb7f8d205fd1d00fe2280a.jpg\n",
      "55 data/coco-128/train/000000000326_jpg.rf.02c19837e58093adec35e737066bb9ae.jpg\n",
      "60 data/coco-128/train/000000000357_jpg.rf.fd60a5947f0f1b2ad6273d8ff87b6282.jpg\n",
      "65 data/coco-128/train/000000000382_jpg.rf.c172a50ccf4da06a423497fac9d12579.jpg\n",
      "70 data/coco-128/train/000000000395_jpg.rf.7b3a9f039340b28eeea8eca51e875130.jpg\n",
      "75 data/coco-128/train/000000000419_jpg.rf.b58c3291ae18177c82e19195fd533614.jpg\n",
      "80 data/coco-128/train/000000000443_jpg.rf.81f83991ba79c61e94912b2d34699024.jpg\n",
      "85 data/coco-128/train/000000000472_jpg.rf.68cb82fda7d6d5abec9e462eb191271a.jpg\n",
      "90 data/coco-128/train/000000000491_jpg.rf.dd6cde4b463637c688bcae9dbb0488f0.jpg\n",
      "95 data/coco-128/train/000000000520_jpg.rf.30218d72a576772917cc478208e40924.jpg\n",
      "100 data/coco-128/train/000000000540_jpg.rf.d42cc5cec9a137294c1d0dd81cacceaf.jpg\n",
      "105 data/coco-128/train/000000000564_jpg.rf.4ec8ed4abf0997c8cb5cea298fef3465.jpg\n",
      "110 data/coco-128/train/000000000584_jpg.rf.65a9bac7029d5afffc477baa9d3b43bc.jpg\n",
      "115 data/coco-128/train/000000000599_jpg.rf.06b89323b92ef324fdd5e7ecf7f20b9b.jpg\n",
      "120 data/coco-128/train/000000000625_jpg.rf.ce871c39393fefd9fd8671806761a1c8.jpg\n",
      "125 data/coco-128/train/000000000641_jpg.rf.2ad873d1d358c17ad3149fac98f1e4bf.jpg\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os \n",
    "from PIL import Image\n",
    "dataset_file_path = \"data/image_paths.json\"\n",
    "with open(dataset_file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "os.makedirs(\"dataset\", exist_ok=True)\n",
    "for i,path in enumerate(data):\n",
    "    image = Image.open(path)\n",
    "    image.save(os.path.join(\"dataset\",f\"{i}.jpg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w3_2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
